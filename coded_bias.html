<!-- This is an HTML page for the JNL221 class' first repository.-->

<!DOCTYPE html>
<html>
	<head>
	
		<meta charset="utf-8">
		<title>First repository</title>
		<link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,400,300,600,700&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
		<link href='https://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>
	
		<!-- This is where this page's style sheet is defined. -->
		<link type="text/css" rel="stylesheet" href="index.css" />

	</head>
	<body>
	
		<!-- This is the header row for this page. -->
		<div id="intro">
			<h1>SOPHIA DOMINICIS</h1>
			<h4>Syracuse University, Fall 2025</h4>
		</div>

		<section>
			<p> Throughout the documentary, there were numerous points at which I discovered a new perspective surrounding artificial intelligence and a growing digital world. First, at the 12:40 mark, the documentary explains that digital intelligence makes legal lines blurry. While typically the police wouldn’t be allowed to take fingerprints at random to use against an individual, they can now use biometric photos stored in a database for the same intended purpose, without much government oversight. Previously, I had not thought about the legal standpoint of new intelligence and simply believed that past laws would expand to include these new modernized tools. Furthermore, I learned that a lot of individuals are fighting against the exploitation of technology, such as facial recognition (19:30). Moreover, I found it extremely interesting that court systems are beginning to use artificial intelligence-based algorithms when sentencing. Since courts and sentencing are viewed as being extremely high stakes, many individuals hope that there are multiple human “safety-nets” thar are thoroughly experienced and trained to properly form these decisions. Now, with the introduction of recidivism risk algorithms, features such as “race, age, gender,” are being used to calculate one’s likelihood of causing more crime (54:00). As stated throughout the documentary, artificial intelligence can often be inherently biased. Now, purposely including these demographic descriptors into the sentencing process opens the door for potentially more injustice in the court system. 
. </p>
 <p> In mainstream media, AI is depicted as this novel creation that is most usually beneficial. In pop culture, artificial intelligence can be seen as futuristic and completely autonomous. 
In reality though, artificial intelligence is something that has been experimented with since the 1950’s and has been slowly learning and modernizing throughout time. Furthermore, as shown in the documentary, artificial intelligence is commonly used for practical purposes, such as crime regulation, rather than narrow futuristic objectives such as serving as a human-like robot companion (which although may be possible, is not currently the most popular usage of AI). 

In terms of algorithms, many cases of pop culture display algorithms as being controlled by humans, with real life individuals seemingly having knowledge of ones every digital move. In media, algorithms can be depicted as a form of spying, and as an invasion of privacy. In the documentary though, artificial intelligence algorithms seem to be more systemic and calculated, rather than personal. In addition, while in pop culture algorithms are primarily depicted as being social media oriented, the documentary reveals that these algorithms are embedded in almost every system, such as college acceptances and credit. 
</p>
 <p>This dataset appears to include a variety of incidents where artificial intelligence has had negative results and has caused harm. Two questions that I would ask I was “interviewing” the dataset would be: 1. What types of environments are most likely to result in a negative artificial intelligence experience, and 2. What guardrails can be put into place to strengthen artificial intelligence filtration systems and protect clients? 

A hypothesis for this topic is: Failures in AI filtration systems that are used to properly evaluate scenarios have the ability to present danger and “hallucinations” due to inaccuracies in their training resources and available content. 
</p>

		</section>

		<div id="end">
			<h4>this page was published on github pages. fonts: montserrat, open sans.</h4>
		</div>


	</body>
</html>
